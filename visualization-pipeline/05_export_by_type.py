#!/usr/bin/env -S uv run
"""
Export embeddings to separate TSV files by data type.

This script:
1. Loads UMAP coordinates and metadata from data files
2. Filters data by type (attribute_labels, descriptions)
3. Creates separate exports in organized directories:
   - tsv_export/attribute_labels/
   - tsv_export/descriptions/
"""

# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "numpy>=1.26.0",
#     "pandas>=2.1.0",
# ]
# ///

import sys
from pathlib import Path

import numpy as np
import pandas as pd


def export_vectors_tsv(coordinates: np.ndarray, output_path: Path):
    """
    Export coordinate vectors to TSV format.

    Format: tab-separated values, no header
    Each row: coord1\tcoord2\t[coord3]
    """
    print(f"  Exporting vectors to: {output_path.name}")

    with open(output_path, 'w') as f:
        for row in coordinates:
            # Format each coordinate with 6 decimal places
            formatted = '\t'.join(f'{x:.6f}' for x in row)
            f.write(f'{formatted}\n')

    print(f"  ✓ Exported {len(coordinates)} vectors")


def export_metadata_tsv(metadata: pd.DataFrame, output_path: Path):
    """
    Export metadata to TSV format.

    Format: tab-separated values with header
    Columns: label, source_column, usage_count, etc.
    """
    print(f"  Exporting metadata to: {output_path.name}")

    # Select and order columns for TensorFlow Projector
    columns_to_export = [
        'label',
        'source_table',
        'source_column',
        'usage_count',
    ]

    # Verify columns exist
    available_columns = [col for col in columns_to_export if col in metadata.columns]

    export_df = metadata[available_columns]

    # Clean up text for TSV format
    for col in export_df.select_dtypes(include=['object']).columns:
        # Replace tabs and newlines
        export_df[col] = export_df[col].astype(str).str.replace('\t', ' ').str.replace('\n', ' ')

    # Export to TSV
    export_df.to_csv(output_path, sep='\t', index=False)

    print(f"  ✓ Exported {len(export_df)} metadata rows")


def create_readme(output_dir: Path, data_type: str, num_vectors: int):
    """
    Create README with usage instructions.
    """
    readme_path = output_dir / "README.md"

    filter_description = {
        'attribute_labels': "Attribute labels from config_position.attribute_label",
        'descriptions': "Descriptions from various tables (*.description)"
    }.get(data_type, data_type)

    content = f"""# TensorFlow Projector Export - {data_type.replace('_', ' ').title()}

This directory contains embeddings exported for visualization in TensorFlow Projector.

## Data Type

**{filter_description}**

Total embeddings: {num_vectors}

## Files

- `vectors_2d.tsv` - 2D UMAP coordinates ({num_vectors} vectors)
- `vectors_3d.tsv` - 3D UMAP coordinates ({num_vectors} vectors)
- `metadata.tsv` - Labels and attributes for each vector

## Usage

### Option 1: TensorFlow Projector (Online)

1. Go to https://projector.tensorflow.org/
2. Click "Load" in the top-left
3. Upload both files:
   - `vectors_2d.tsv` or `vectors_3d.tsv` (as "Load data")
   - `metadata.tsv` (as "Load metadata")
4. The visualization will appear automatically

### Option 2: Local TensorBoard

```bash
# Install TensorBoard
pip install tensorboard

# Create projector config
cat > projector_config.pbtxt <<EOF
embeddings {{
  tensor_name: "{data_type.replace('_', ' ').title()}"
  tensor_path: "vectors_2d.tsv"
  metadata_path: "metadata.tsv"
}}
EOF

# Run TensorBoard
tensorboard --logdir=. --port=6006

# Open in browser: http://localhost:6006
```

## Visualization Tips

1. **Color by metadata**: Use the "Color by" dropdown to color points by source_column or usage_count
2. **Search**: Type a label in the search box to highlight specific embeddings
3. **Nearest neighbors**: Click a point to see its nearest neighbors
4. **Clustering**: Similar labels should cluster together
5. **Isolate points**: Select points and use "Isolate selection" to focus on specific clusters

## Data Details

- **Total embeddings**: {num_vectors}
- **Original dimension**: 1536d
- **Reduced dimensions**: 2D/3D (via UMAP)
- **UMAP parameters**:
  - n_neighbors: 20
  - min_dist: 0.1
  - metric: cosine
  - random_state: 42

## Generated

This export was generated by the zebra-h2b-audit-v2 visualization pipeline.
"""

    readme_path.write_text(content)
    print(f"  ✓ Created README: {readme_path.name}")


def export_data_type(
    data_type: str,
    df_2d: pd.DataFrame,
    df_3d: pd.DataFrame,
    output_base: Path
):
    """
    Export a specific data type to its own directory.

    Args:
        data_type: Name of the data type (used for directory name)
        df_2d: DataFrame with 2D coordinates and metadata
        df_3d: DataFrame with 3D coordinates and metadata
        output_base: Base output directory
    """
    print(f"\n{'=' * 80}")
    print(f"EXPORTING: {data_type.upper()}")
    print(f"{'=' * 80}")

    # Create output directory
    output_dir = output_base / data_type
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Output directory: {output_dir}")

    # Extract just the coordinates
    coords_2d = df_2d[['umap_x', 'umap_y']].values
    coords_3d = df_3d[['umap_x', 'umap_y', 'umap_z']].values

    # Export 2D vectors
    print("\n2D Vectors:")
    vectors_2d_path = output_dir / "vectors_2d.tsv"
    export_vectors_tsv(coords_2d, vectors_2d_path)

    # Export 3D vectors
    print("\n3D Vectors:")
    vectors_3d_path = output_dir / "vectors_3d.tsv"
    export_vectors_tsv(coords_3d, vectors_3d_path)

    # Export metadata (same for both 2D and 3D)
    print("\nMetadata:")
    metadata_path = output_dir / "metadata.tsv"
    export_metadata_tsv(df_2d, metadata_path)

    # Create README
    print("\nREADME:")
    create_readme(output_dir, data_type, len(df_2d))

    # File sizes
    size_2d = vectors_2d_path.stat().st_size / 1024
    size_3d = vectors_3d_path.stat().st_size / 1024
    size_meta = metadata_path.stat().st_size / 1024

    print(f"\nFile sizes:")
    print(f"  - vectors_2d.tsv: {size_2d:.1f} KB")
    print(f"  - vectors_3d.tsv: {size_3d:.1f} KB")
    print(f"  - metadata.tsv: {size_meta:.1f} KB")

    print(f"\n✓ Export complete for {data_type}")


def main():
    """Main execution flow."""
    print("=" * 80)
    print("TSV EXPORT BY DATA TYPE")
    print("=" * 80)
    print()

    # Load data
    data_dir = Path(__file__).parent / "data"
    umap_2d_csv = data_dir / "umap_2d_with_metadata.csv"
    umap_3d_csv = data_dir / "umap_3d_with_metadata.csv"

    if not all([umap_2d_csv.exists(), umap_3d_csv.exists()]):
        print("❌ Error: Required files not found")
        print("Please run the previous scripts first:")
        print("  1. 01_extract_embeddings.py")
        print("  2. 02_reduce_with_umap.py")
        sys.exit(1)

    print("Loading data...")
    df_2d_all = pd.read_csv(umap_2d_csv)
    df_3d_all = pd.read_csv(umap_3d_csv)

    print(f"✓ Loaded 2D data: {df_2d_all.shape}")
    print(f"✓ Loaded 3D data: {df_3d_all.shape}")

    # Show breakdown
    print("\nData breakdown:")
    breakdown = df_2d_all.groupby(['source_table', 'source_column']).size()
    for (table, column), count in breakdown.items():
        print(f"  {table}.{column}: {count}")

    # Create base output directory
    output_base = Path(__file__).parent / "tsv_export"
    output_base.mkdir(exist_ok=True)

    # Export attribute labels
    print("\n" + "=" * 80)
    print("FILTERING: Attribute Labels")
    print("=" * 80)
    attr_filter = (df_2d_all['source_table'] == 'config_position') & \
                  (df_2d_all['source_column'] == 'attribute_label')
    df_2d_attr = df_2d_all[attr_filter].copy()
    df_3d_attr = df_3d_all[attr_filter].copy()
    print(f"Filtered {len(df_2d_attr)} attribute labels")

    if len(df_2d_attr) > 0:
        export_data_type('attribute_labels', df_2d_attr, df_3d_attr, output_base)
    else:
        print("⚠️  No attribute labels found to export")

    # Export descriptions
    print("\n" + "=" * 80)
    print("FILTERING: Descriptions")
    print("=" * 80)
    desc_filter = df_2d_all['source_column'] == 'description'
    df_2d_desc = df_2d_all[desc_filter].copy()
    df_3d_desc = df_3d_all[desc_filter].copy()
    print(f"Filtered {len(df_2d_desc)} descriptions")

    if len(df_2d_desc) > 0:
        export_data_type('descriptions', df_2d_desc, df_3d_desc, output_base)
    else:
        print("⚠️  No descriptions found to export")

    # Summary
    print("\n" + "=" * 80)
    print("EXPORT SUMMARY")
    print("=" * 80)
    print(f"\nTotal rows processed: {len(df_2d_all)}")
    print(f"Attribute labels exported: {len(df_2d_attr)}")
    print(f"Descriptions exported: {len(df_2d_desc)}")
    print(f"\nExport directory: {output_base}")
    print("\nDirectory structure:")
    print("tsv_export/")
    print("├── attribute_labels/")
    print("│   ├── vectors_2d.tsv")
    print("│   ├── vectors_3d.tsv")
    print("│   ├── metadata.tsv")
    print("│   └── README.md")
    print("└── descriptions/")
    print("    ├── vectors_2d.tsv")
    print("    ├── vectors_3d.tsv")
    print("    ├── metadata.tsv")
    print("    └── README.md")
    print()
    print("✓ All exports complete!")
    print("\nNext steps:")
    print("  1. Open https://projector.tensorflow.org/")
    print("  2. Load the TSV files for the data type you want to visualize")
    print("  3. Explore the visualization to identify patterns and clusters")


if __name__ == "__main__":
    main()
